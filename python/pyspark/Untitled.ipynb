{
 "cells": [
  {
   "cell_type": "code",
   "id": "3a936785-cd86-4111-b8c8-6674cf40636f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T01:30:09.033852700Z",
     "start_time": "2026-01-30T01:29:54.123084800Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803781b8-7401-41a0-825f-f0cf9bb54879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 72, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 540, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 630, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 503, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 156, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 236, in <setcomp>\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\serializers.py:437\u001B[39m, in \u001B[36mCloudPickleSerializer.dumps\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    436\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m437\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcloudpickle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    438\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m pickle.PickleError:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:72\u001B[39m, in \u001B[36mdumps\u001B[39m\u001B[34m(obj, protocol, buffer_callback)\u001B[39m\n\u001B[32m     69\u001B[39m cp = CloudPickler(\n\u001B[32m     70\u001B[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001B[32m     71\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m \u001B[43mcp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m file.getvalue()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:540\u001B[39m, in \u001B[36mCloudPickler.dump\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    539\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:630\u001B[39m, in \u001B[36mCloudPickler.reducer_override\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    629\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, types.FunctionType):\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_function_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    632\u001B[39m     \u001B[38;5;66;03m# fallback to save_global, including the Pickler's\u001B[39;00m\n\u001B[32m    633\u001B[39m     \u001B[38;5;66;03m# dispatch_table\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:503\u001B[39m, in \u001B[36mCloudPickler._function_reduce\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    502\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m503\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dynamic_function_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:484\u001B[39m, in \u001B[36mCloudPickler._dynamic_function_reduce\u001B[39m\u001B[34m(self, func)\u001B[39m\n\u001B[32m    483\u001B[39m newargs = \u001B[38;5;28mself\u001B[39m._function_getnewargs(func)\n\u001B[32m--> \u001B[39m\u001B[32m484\u001B[39m state = \u001B[43m_function_getstate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (types.FunctionType, newargs, state, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    486\u001B[39m         _function_setstate)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:156\u001B[39m, in \u001B[36m_function_getstate\u001B[39m\u001B[34m(func)\u001B[39m\n\u001B[32m    145\u001B[39m slotstate = {\n\u001B[32m    146\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m__name__\u001B[39m\u001B[33m\"\u001B[39m: func.\u001B[34m__name__\u001B[39m,\n\u001B[32m    147\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m__qualname__\u001B[39m\u001B[33m\"\u001B[39m: func.\u001B[34m__qualname__\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    153\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m__closure__\u001B[39m\u001B[33m\"\u001B[39m: func.\u001B[34m__closure__\u001B[39m,\n\u001B[32m    154\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m f_globals_ref = \u001B[43m_extract_code_globals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__code__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    157\u001B[39m f_globals = {k: func.\u001B[34m__globals__\u001B[39m[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m f_globals_ref \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m\n\u001B[32m    158\u001B[39m              func.\u001B[34m__globals__\u001B[39m}\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:236\u001B[39m, in \u001B[36m_extract_code_globals\u001B[39m\u001B[34m(co)\u001B[39m\n\u001B[32m    235\u001B[39m names = co.co_names\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m out_names = \u001B[43m{\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m[\u001B[49m\u001B[43moparg\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moparg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_walk_global_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43mco\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001B[39;00m\n\u001B[32m    239\u001B[39m \u001B[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[38;5;66;03m# of the nested function's As the nested function may itself need\u001B[39;00m\n\u001B[32m    241\u001B[39m \u001B[38;5;66;03m# global variables, we need to introspect its code, extract its\u001B[39;00m\n\u001B[32m    242\u001B[39m \u001B[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001B[39;00m\n\u001B[32m    243\u001B[39m \u001B[38;5;66;03m# add the result to code_globals\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:236\u001B[39m, in \u001B[36m<setcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    235\u001B[39m names = co.co_names\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m out_names = {\u001B[43mnames\u001B[49m\u001B[43m[\u001B[49m\u001B[43moparg\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _, oparg \u001B[38;5;129;01min\u001B[39;00m _walk_global_ops(co)}\n\u001B[32m    238\u001B[39m \u001B[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001B[39;00m\n\u001B[32m    239\u001B[39m \u001B[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[38;5;66;03m# of the nested function's As the nested function may itself need\u001B[39;00m\n\u001B[32m    241\u001B[39m \u001B[38;5;66;03m# global variables, we need to introspect its code, extract its\u001B[39;00m\n\u001B[32m    242\u001B[39m \u001B[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001B[39;00m\n\u001B[32m    243\u001B[39m \u001B[38;5;66;03m# add the result to code_globals\u001B[39;00m\n",
      "\u001B[31mIndexError\u001B[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mPicklingError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m employees = [{\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mJohn D.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mage\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m30\u001B[39m},\n\u001B[32m      2\u001B[39m   {\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mAlice G.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mage\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m25\u001B[39m},\n\u001B[32m      3\u001B[39m   {\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mBob T.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mage\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m35\u001B[39m},\n\u001B[32m      4\u001B[39m   {\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mEve A.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mage\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m28\u001B[39m}]\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Create a DataFrame containing the employees data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m df = \u001B[43mspark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43memployees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m df.show()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:675\u001B[39m, in \u001B[36mSparkSession.createDataFrame\u001B[39m\u001B[34m(self, data, schema, samplingRatio, verifySchema)\u001B[39m\n\u001B[32m    671\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pandas.DataFrame):\n\u001B[32m    672\u001B[39m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[32m    673\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m).createDataFrame(\n\u001B[32m    674\u001B[39m         data, schema, samplingRatio, verifySchema)\n\u001B[32m--> \u001B[39m\u001B[32m675\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:701\u001B[39m, in \u001B[36mSparkSession._create_dataframe\u001B[39m\u001B[34m(self, data, schema, samplingRatio, verifySchema)\u001B[39m\n\u001B[32m    699\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    700\u001B[39m     rdd, schema = \u001B[38;5;28mself\u001B[39m._createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[32m--> \u001B[39m\u001B[32m701\u001B[39m jrdd = \u001B[38;5;28mself\u001B[39m._jvm.SerDeUtil.toJavaArray(\u001B[43mrdd\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_to_java_object_rdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    702\u001B[39m jdf = \u001B[38;5;28mself\u001B[39m._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())\n\u001B[32m    703\u001B[39m df = DataFrame(jdf, \u001B[38;5;28mself\u001B[39m._wrapped)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2618\u001B[39m, in \u001B[36mRDD._to_java_object_rdd\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2612\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\" Return a JavaRDD of Object by unpickling\u001B[39;00m\n\u001B[32m   2613\u001B[39m \n\u001B[32m   2614\u001B[39m \u001B[33;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001B[39;00m\n\u001B[32m   2615\u001B[39m \u001B[33;03mRDD is serialized in batch or not.\u001B[39;00m\n\u001B[32m   2616\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2617\u001B[39m rdd = \u001B[38;5;28mself\u001B[39m._pickled()\n\u001B[32m-> \u001B[39m\u001B[32m2618\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ctx._jvm.SerDeUtil.pythonToJava(\u001B[43mrdd\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jrdd\u001B[49m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2949\u001B[39m, in \u001B[36mPipelinedRDD._jrdd\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2946\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2947\u001B[39m     profiler = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2949\u001B[39m wrapped_func = \u001B[43m_wrap_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prev_jrdd_deserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2950\u001B[39m \u001B[43m                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jrdd_deserializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofiler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2951\u001B[39m python_rdd = \u001B[38;5;28mself\u001B[39m.ctx._jvm.PythonRDD(\u001B[38;5;28mself\u001B[39m._prev_jrdd.rdd(), wrapped_func,\n\u001B[32m   2952\u001B[39m                                      \u001B[38;5;28mself\u001B[39m.preservesPartitioning, \u001B[38;5;28mself\u001B[39m.is_barrier)\n\u001B[32m   2953\u001B[39m \u001B[38;5;28mself\u001B[39m._jrdd_val = python_rdd.asJavaRDD()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2828\u001B[39m, in \u001B[36m_wrap_function\u001B[39m\u001B[34m(sc, func, deserializer, serializer, profiler)\u001B[39m\n\u001B[32m   2826\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m serializer, \u001B[33m\"\u001B[39m\u001B[33mserializer should not be empty\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2827\u001B[39m command = (func, profiler, deserializer, serializer)\n\u001B[32m-> \u001B[39m\u001B[32m2828\u001B[39m pickled_command, broadcast_vars, env, includes = \u001B[43m_prepare_for_python_RDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2829\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m sc._jvm.PythonFunction(\u001B[38;5;28mbytearray\u001B[39m(pickled_command), env, includes, sc.pythonExec,\n\u001B[32m   2830\u001B[39m                               sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2814\u001B[39m, in \u001B[36m_prepare_for_python_RDD\u001B[39m\u001B[34m(sc, command)\u001B[39m\n\u001B[32m   2811\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_prepare_for_python_RDD\u001B[39m(sc, command):\n\u001B[32m   2812\u001B[39m     \u001B[38;5;66;03m# the serialized command will be compressed by broadcast\u001B[39;00m\n\u001B[32m   2813\u001B[39m     ser = CloudPickleSerializer()\n\u001B[32m-> \u001B[39m\u001B[32m2814\u001B[39m     pickled_command = \u001B[43mser\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2815\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001B[38;5;66;03m# Default 1M\u001B[39;00m\n\u001B[32m   2816\u001B[39m         \u001B[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001B[39;00m\n\u001B[32m   2817\u001B[39m         broadcast = sc.broadcast(pickled_command)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pyspark\\serializers.py:447\u001B[39m, in \u001B[36mCloudPickleSerializer.dumps\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    445\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mCould not serialize object: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % (e.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, emsg)\n\u001B[32m    446\u001B[39m print_exec(sys.stderr)\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m pickle.PicklingError(msg)\n",
      "\u001B[31mPicklingError\u001B[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "employees = [{\"name\": \"John D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "# Create a DataFrame containing the employees data\n",
    "df = spark.createDataFrame(employees)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7679a9-1809-4b6c-922f-179b83e748ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
